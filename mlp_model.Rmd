---
title: "mlp_fragrance"
author: "Mimi Perez"
date: "2025-04-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(dplyr)
library(caret)  # For data splitting
library(data.table)  # For one-hot encoding
library(scales)  # For Min-Max scalingy
```


```{r}
data <- read.csv("mlp_perfume.csv")
colnames(data)
```

```{r}
# get rid of columns i will not be using
data <- subset(data, select = -c(perfume, brand, year, mainaccord3, mainaccord4, notes))
colnames(data)
```


```{r}
# one hot encoding the dataset
onehot_data <- dummyVars("~ .", data = data, fullRank = TRUE)
onehot_data <- predict(onehot_data, newdata = data)
onehot_data <- as.data.frame(onehot_data)

head(onehot_data)


# convert the one-hot encoded data to a numeric matrix
onehot_data <- as.matrix(onehot_data)

# min max scaling
X <- onehot_data
X_scaled <- apply(X, 2, function(x) rescale(x, to = c(0, 1)))
head(X_scaled)

# add bias column
X_scaled <- cbind(X_scaled, Bias = 1)
head(X_scaled)
colnames(X_scaled)

# separating vectors
X <- X_scaled[, - (ncol(X_scaled) - 1)] # Gender, Rating Val, Rating Count, Mainaccord1, Main Accord 2
y <- X_scaled[, ncol(X_scaled) - 1]  # target: luxury_or_cheap (second to last col)

# confirm y array
is.vector(y)

# split the data
set.seed(42)
index <- createDataPartition(y, p = 0.7, list = FALSE)
X_train <- X[index, ]
X_test <- X[-index, ]
y_train <- y[index]
y_test <- y[-index]

# Training data
dim(X_train)
length(y_train)

# Testing data
dim(X_test)
length(y_test)
```

## looking at class balances
```{r}
table(data$luxury_or_cheap)
```


## training model and calculating errors
```{r}
# learning rate
eta <- 0.1
epochs <- 800

# intialize weights
input_size <- ncol(X_train)
hidden_size <- 20 
output_size <- 1

# intialize weights 
# the rows of W1 correspond to the columns of X, the columns to the number of hidden nodes
# the rows of W2 correspond to the number of hidden nodes, the columns to the dimension of the output y
W1 <- matrix(rnorm(input_size * hidden_size), nrow = input_size, ncol = hidden_size)
W2 <- matrix(rnorm(hidden_size * output_size), nrow = hidden_size, ncol = output_size)


# define relu activation function
relu <- function(x) {
  matrix(pmax(0, x))
}

# Define Sigmoid activation function
sigmoid <- function(x) {
  1 / (1 + exp(-x))
}


# Define the forward pass
f <- function(x) {
  # Input to hidden layer
  h1 <- relu(t(W1) %*% x)
  
  # Hidden to output layer
  z2 <- t(W2) %*% h1
 # z2 <- W2 %*% h1
  y_hat <- sigmoid(z2)  # Output layer activation (Sigmoid)
  
  return(y_hat)
}

# Gradient descent
errors <- numeric()
n <- nrow(X_train)

for (epoch in 1:epochs) {
  # Update W2
  dW2 <- matrix(0, nrow = hidden_size, ncol = output_size)
  for (i in 1:n) {
    x <- matrix(X_train[i, ], nrow = input_size, ncol = output_size)
    h <- relu(t(W1) %*% x)

    error <- (f(x) - y_train[i])[1]  # Error term for output layer
    
    sigmoid_derivative <- sigmoid(t(W2) %*% h) * (1 - sigmoid(t(W2) %*% h))
    
  
    
    # Backpropagate
    dW2 <- dW2 + (2 / n) * error * h
  }
  W2 <- W2 - eta * dW2
  
  # Update W1
  dW1 <- matrix(0, nrow = input_size, ncol = hidden_size)
  for (i in 1:n) {
    x <- matrix(X_train[i, ], nrow = input_size, ncol = output_size)
    h <- relu(t(W1) %*% x)
  
    sigmoid_derivative <- sigmoid(x) * (1 - sigmoid(x))
    
    
    mat1 <- ifelse(h > 0, 1, 0)  # ReLU derivative
    
    # Backpropagate the error through W1
    dW1 <- dW1 + (2 / n) * (f(x) - y_train[i])[1] * kronecker(t(W2 * mat1), x)
    
  }
  W1 <- W1 - eta * dW1
  
  # Compute error
  e <- (1 / n) * sum((apply(X_train, MARGIN = 1, FUN = f) - y_train)^2)
  errors <- c(errors, e)
}

# Print predictions
#print(apply(X_train, MARGIN = 1, FUN = f))

# Plot the errors
plot(1:epochs, errors, type = "l", main = "Error over Epochs (MSE)", xlab = "Epochs", ylab = "Error (MSE)")



```

## Final Weights
```{r}
# Print the final weights
cat("Final Weights for W1 (Input to Hidden):\n")
print(W1)

cat("Final Weights for W2 (Hidden to Output):\n")
print(W2)
```


## Accuracy
```{r}
# Make predictions on the test set
y_pred <- apply(X_test, MARGIN = 1, FUN = function(x) f(matrix(x, nrow = input_size, ncol = 1)))

# Convert predictions to binary (0 or 1)
y_pred_binary <- ifelse(y_pred > 0.5, 1, 0)

# Compute accuracy
accuracy <- mean(y_pred_binary == y_test)
cat("Accuracy on the test set:", accuracy, "\n")


```
### with a learning rate of .1, hidden_size = 10: accuracy on the test set: 0.6172718
### with a learning rate of .01, hidden_size = 10: accuracy on the test set: 0.5495584 

## with a learning rate of .1, hidden_size = 5: accuracy on the test set: 0.6015702 
## with a learning rate of .1, hidden_size = 20: accuracy on the test set: 0.6683023, but converges slower
## with a learning rate of .1, hidden_size = 30: accuracy on the test set: 0.6781158, 
## with a learning rate of .1, hidden_size = 15: accuracy on the test set: 0.6408243 


## with a learning rate of .1, hidden_size = 20, epochs = 800: accuracy on the test set: 0.6908734, but goes converges slower


## Log-Loss
```{r}
# log-loss function
log_loss <- function(y_true, y_pred) {
  epsilon <- 1e-15  # small value to avoid log(0)
  y_pred <- pmax(pmin(y_pred, 1 - epsilon), epsilon)  # clipping predictions to avoid extreme values
  loss <- -mean(y_true * log(y_pred) + (1 - y_true) * log(1 - y_pred))
  return(loss)
}
    

# Calculate the loss on the test set
test_loss <- log_loss(y_test, y_pred)
cat("Test Log-Loss:", test_loss, "\n")
```


# Precision and Recall
```{r}

# precision and recall
precision <- sum(y_pred_binary == 1 & y_test == 1) / sum(y_pred_binary == 1)
recall <- sum(y_pred_binary == 1 & y_test == 1) / sum(y_test == 1)


cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")

```



```{r}
# Combine them into a data frame
results <- data.frame(True_Labels = y_test, Predicted_Labels = y_pred_binary)

# Write the data frame to a CSV file
write.csv(results, "y_true_y_pred.csv", row.names = FALSE)
```

